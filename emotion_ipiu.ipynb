{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image \n",
    "%matplotlib inline \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt \n",
    "import torch \n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import numpy as np \n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torchvision.datasets as dsets\n",
    "import torch.nn.init\n",
    "from torchsummary import summary\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import time\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = transforms.Compose([transforms.Grayscale(num_output_channels=1),\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize((0.5,),(0.5,))\n",
    "                            ])\n",
    "trainset = torchvision.datasets.ImageFolder(root=\"C:/Users/1315/Desktop/expression/face/far2013/train\",\n",
    "                                            transform = trans)\n",
    "\n",
    "\n",
    "testset = torchvision.datasets.ImageFolder(root=\"C:/Users/1315/Desktop/expression/face/far2013/test\",\n",
    "                                            transform = trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(trainset,\n",
    "                        batch_size=1,\n",
    "                        shuffle=True,\n",
    "                        num_workers=4)\n",
    "                    \n",
    "testloader = DataLoader(testset,\n",
    "                        batch_size=1,\n",
    "                        shuffle=False,\n",
    "                        num_workers=4)\n",
    "\n",
    "classes = testset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataiter = iter(trainloader)\n",
    "test_dataiter = iter(testloader)\n",
    "\n",
    "train_images,train_labels = train_dataiter.next()\n",
    "test_images,test_labels = test_dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 48, 48])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD6CAYAAABnLjEDAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAiaUlEQVR4nO2da6xfVZnGn7enLXctvR96oS2WSy0DmAodIKYWaxhQUaKJTpwwCYYvMwlmGKXMJJP4YZJOJjF+mPnSRGMnGI0JJlQiasNFYgClRW5tqaf0RumVUhS8YEvXfDj/g2c/6zlnrx7a//nDen5Jc7r2WXvttdfe7/mf9znv+65IKcEY8/5nwnhPwBjTHWzsxlSCjd2YSrCxG1MJNnZjKsHGbkwlvCtjj4gbI2JbRGyPiNWnalLGmFNPjPXv7BHRB+C3AFYB2AvgKQBfSiltGemcM844I5111lmNY+eee26jPXHixOy8CRPafyZxn4jI+vC9qns/ceJE63z42PHjx1vno+ak+hw7dqzRfuutt7I+fX19jfbkyZNb51iCWjMFrxG3gbK15mNqnLZrK/785z9nx84888zW89Ra8zP64x//mPXhY3/5y1+yPvw83n777db5lKzZCOfJB3nyb8RfuRrA9pTSDgCIiB8AuAXAiMZ+1llnYcWKFY1j1157baM9derU7LxzzjmndTL8MJUB8ENQRsoPTs1n1qxZjfahQ4eyPmrOZ5xxRqN99tlnZ33279/faA8MDGR9zj///EZ73rx5WR+eN/+AAHLDmTRpUtZHvVy8Rn/605+yPry2/EMMyI1SGRsbhTIk7rN9+/asz+LFi7NjzM6dO7Nj/OG0adOmrM8zzzzTaO/ZsyfrM3369Eb7d7/7Xet81PvJa8Y/oEf7YfBufo2fA+DlYe29nWPGmB7k3Ri7+lUh+7ESEXdExMaI2Kh+KhtjusO7Mfa9AIb//jgXwD7ulFJam1JallJapn61NsZ0h3fjsz8FYHFELATwCoAvAvj70U6YPHkyLrjggsax8847r9FmHwnIxQ0lPrE/rH6wsNiiBDL2k9g/B4A//OEPreOo6/O9HjhwIOvz+9//vtGeNm1a1mfOnKa3NGXKlKyP8tEZ9veU+FUiJJUIe2qNWCNQ/ib7+upa7OuzpgHka6/uS90/X2/mzJlZnw9+8IONttJi+HqqD98Hi9dArg+VCJZDjNnYU0rHI+KfAfwMQB+A76SUNo91PGPM6eXdfLIjpfQTAD85RXMxxpxGHEFnTCW8q0/2k2Xy5MmYP39+41hJoAn7n+x/AWVBJDy2+tsv+3vKt3rttdcabf4bKpBrCACwe/fuRlv5W3wfM2bMyPrw9Ur8cwX7yMqPVcf4POVHlzxX9Xf9tnFUwAz3UQE0fF6JNgTkMQTq3WPfWvn1Bw8ebLSVztI2LpBrSkePHm20R9NY/MluTCXY2I2pBBu7MZVgYzemEroq0E2cODETl1jIKhFtOIgByEUjlVTBYhuLJkCeQKIED76+EoReeeWV7BiHC6vAGw6iKQmYUcEoLIiVZPiVBNAolEDHcxxr4A0fU2IkH1N9WMhSAp0S3zgxSQm2LKSpJCgW/1T2nBJ1GR77zTffbLRHC7LxJ7sxlWBjN6YSbOzGVEJXffYJEyZk/i0HsXCSCZD7O8rfYh9I+f58TPna7BO98cYbWR/2rVTRAxWww+epIA6+NzXHEn+cUb5cSRJFSZWgsZ7HASLqPtjXV2vGWojy2dm35Tagg1gYNcfZs2c32irBifsoTYffj5KAppIKTe/0HfE7xpj3FTZ2YyrBxm5MJdjYjamErgp0EZGJEBz8ooQTrgyqRBoWxFTgDVf0VJVjWSBT19q3r1l9q0RUBPIgGhWgwX2U2MSiTEmFGSXcjKX8dik8VmmZ6rZx1Hz4XpU4yO+ZEtFUthoHNXHGI5C/M+q58nNUFZCOHDnSaCuRmd/zkpLdQ/iT3ZhKsLEbUwk2dmMqYdx9duUDMew3qYAV9olUMAr7RCUVVlRQzeHDhxttrggL6AQW9uVUcBAH9ZT40aoP+/HK9y/Zfqmk4qrykUt8/ZMJCBntWqyPlPRRz1U9M64UpCrllFSz4aAvVYGIx1b6FY/T5sMPx5/sxlSCjd2YSrCxG1MJNnZjKqGrAh2QB0CwcKREK85qUiIJB6MoEY/7qG2CeGy1HfPWrVsb7UWLFmV95s6dmx37wAc+0Gire+U5qm17SwQypqRSTelWQiXXL6l6U7IdV9v7MtJ5DIujSvx6/fXXs2Ocrdbf35/1YdFMVSBiwVgFzCxcuLDRVtt1sxh8MkFP/mQ3phJs7MZUgo3dmEroqs+eUsp8UPZ3Srbk5YQWIPeR2c8H8qAJVc2TfaInn3wy68P+n/LZS5JsFOyDqYSaEl+b7035/iVJFOp58JzGGnhT4o+XbCvN81HPlTWckm29gLzarzqPtZiSYC31PHg7c6UrcEVkXjNXlzXG2NiNqQUbuzGVYGM3phK6KtAdP34cr776auNYiUjD56igGg5SUAIIC2Sqz7Zt20a9NgB8/OMfb7RVhRNVlrikBPRYKsOMdRwWjUoy4xRjzXpjYU2JkTxOSUCVeq4sXKmgFnUel3xeunRp1ofHUtlzJcEwnPE4f/78rM9zzz3XOs5I+JPdmEqwsRtTCa3GHhHfiYhDEfHCsGNTI2JDRAx0vuZB5saYnqLEZ/8ugP8B8H/Djq0G8FBKaU1ErO60724b6MSJE1kQDftJKomAg2qUP1wSIMI+qQq82bFjR6Ot/K9LLrmk0S5JaAFy31b5iCWwzqGSfrhPSeCLCgZRY5ecp/xvhu9frUfJ+8HBJ2q77pL3Q815z549jTYnxgB5UI3SA0q2leb3SAVmcdVklag1Eq2f7CmlxwBwaNEtANZ1/r8OwGeLr2iMGRfG6rPPSintB4DO11yONsb0FKddoIuIOyJiY0RsVJspGGO6w1iN/WBE9ANA5+uIjkNKaW1KaVlKaRn/HdEY0z3GGlSzHsBtANZ0vt5fclJKKRN8VBlmhgUQVS2EBSklkrBApiqTcLnpOXPmtF5LoQQxvvcSEWusFWZYNFPrwfeh7qtkjiXlrsdSuUaNrTLB+FolQp8SZ1X5cb7eli1bsj5LlixptFVmHIvKJRWI5s2bl/XhDMtTKtBFxPcBPAHgkojYGxG3Y9DIV0XEAIBVnbYxpodp/bGdUvrSCN+64RTPxRhzGnEEnTGV0NVEmLfffjurMsPVQZTfxNvrKN+OfSAlBvJ5XPUDyCt6qiCOxx57rNGePn161kdtycvag4L7qK2E1JwYXg/lM7OvX9IHyANCSqq7lvjRKoCHn70ah3WfXbt2ZX04oUX5uqpSDW/1pZ41a0gqEIv/EsXaEJBXbVLVj9lnV5WURsKf7MZUgo3dmEqwsRtTCTZ2Yyqh66Wk24JqlLBVksHFx0rK+e7fvz/rc/To0UZblSVm0YxFJADYtGlTdozvXQlSfB9qGykOtlDZURzEoe6D11UFmii4X0m1FHWvnJ3GAhWQlw3fvn171ocDXThzEcgFOSVGqvvn9VciK+/1rtaan4e6PgfwqIAmDvJisXS04CV/shtTCTZ2YyrBxm5MJdjYjamErgp0J06cyEoBl4g0LIqoiC0+psr+sODB0VFAHg3FGU1AnkGmxCcV6cWCoBL2eN4q0ouFHFWmi/coUyIeC0kloiaQC3JKMOX7Z+ETyNfj5ZdfzvpwWSiOhAPy7EX17Hk9uLwToMW3GTNmNNolpcSUSMYRnWqO/FyV8MkRfPzsVVbgEP5kN6YSbOzGVIKN3ZhK6KrPfuzYscznYr9VZR5x9o8KfmA/qWQrIeWP8vZPu3fvzvrwecqvVhV4SgJ/WA9QfiQfU5mCvEbKZ2Z/T2XYqbVmv1VpD3w95Wuzz64ywdT2Wwz7w8pnLinjrbQXnqPSR1iHUnoAP6MSvUbNh98Pno/SvIbwJ7sxlWBjN6YSbOzGVIKN3ZhK6HpZKi7PU1JymIULJeKxuKQCElgQUyWGWFj76U9/2notJYqoQBMuJ6XEr507dzba6j6uuOKKRpv3ngOAhQsXNtoqqIZLHiuhT5WgPnDgQKOtynuxaMnnqD4qIIQDn5RoxeKbEkf5mNrnXYlvHFSj1oj7qMzNkr3mWDRUa88okXck/MluTCXY2I2pBBu7MZXQVZ99woQJmZ/IfqvaW5v9q71792Z92P8u2TZJba/zoQ99qNHmRAwgD+JQQRTKJ+QAHRWMwz6pGocDVlRllosvvrjRvu6667I+y5cvb7RVUI0K/mAfWd3Hiy++2DpHrvCitk3i58p7mAN5Iox69jxnpamooB6etyoTzWMpDYf1AJXMpZJjGB5b3cdI+JPdmEqwsRtTCTZ2YyrBxm5MJXRVoJs0aVIWgMClgpUgxCKZEklYtFJZTRykMHv27KzP0qVLW+ezdevW1vl87Wtfy45xeen169dnfTiw5PLLL8/6XH/99Y02i2FALvTxOiuUQFYSHKQy6h5//PFGWwmvV155ZaOt9udj0XDjxo1ZHw6yUiXCOQtSBdAo+D1S55Xsc8/ryKIikFcOUuvBQWk8HyUyDuFPdmMqwcZuTCXY2I2phK767H19fVnVGfbl2CcB8iSKkoAIFYzCQQuqCsw111zTaM+cOTPrs2DBgkZb+azKJ7v11ltbz2Of/XOf+1zWh/1vVZWHEzYuvPDCrA/7iCVVe4E8aEXdK+ssF110UdaHg5GUr8trxklAALB27dpGWyXm8LZJKqGFqxQBuT6hkqfYt1ZrxsFjHFAE5IFQan92thdew9ECc/zJbkwl2NiNqQQbuzGV0GrsETEvIh6JiK0RsTki7uwcnxoRGyJioPM1dzCMMT1DiUB3HMBdKaWnI+I8AJsiYgOAfwTwUEppTUSsBrAawN2jDdTX15eJYhwQofa25gARVdGEK5GoIA4WW1TVExZJlJDDIsiTTz6Z9XnkkUeyYyxAff7zn8/6lOwhz9skKVGGBTGuXAPkQTSq6om6/5Ky3R/72McabSV08vU44ArIM+o4oAjI35nnn38+68PvhwrOUXDgFQddAfl7pSrM8PuoSmuvXLmydRwO8uJxVcbfEK2f7Cml/Smlpzv/fwPAVgBzANwCYF2n2zoAn20byxgzfpyUzx4RCwBcBeBXAGallPYDgz8QAOQ/ugfPuSMiNkbERvVnNWNMdyg29og4F8B9AL6aUsor+o1ASmltSmlZSmmZivU1xnSHoqCaiJiEQUP/XkrpR53DByOiP6W0PyL6AeTlSghVqYbbJQEz7CMBuS+j/Hr2P5XPXlKtkyu1qkAPFTTBlWNfeuml1uuXVDxVlWM5iEb1KdmOqiTQRlVT5bXmrbCB/N5UUA0nHT3xxBNZH15r9exZ51AViNR7xT76/Pnzsz6MqqbDgT5qe2r+MCzZnprtRz2vd7434nc6xOAb8G0AW1NK3xz2rfUAbuv8/zYA97eNZYwZP0o+2a8D8A8Ano+IZzrH/g3AGgA/jIjbAewB8IXTMkNjzCmh1dhTSr8EMNLvtjec2ukYY04XjqAzphK6mvUG5H/0Z1FCBXGU7H/N4o6qzMLCidomiLOalGjFgUEqqEWJTRx8ojLzWGBR6zFt2rRGW1XcUWWhGRboSva9V3NSghTfK88ZyAUxFQjFIp4qI86ilSptzagts5TQuGjRokZbZb2x0KveGf6zs8po4/UfGBhoHYeDwNQ5Q/iT3ZhKsLEbUwk2dmMqoas+e0opq9bJSQwqiIQTApSvy36k8j/5PLVND1ePUf4o+5qq4o1KSGA/ViU6sP+v1oPHUVVhGRVAxPehAjKUH81+vFprXlv1zFhXUPfB9698Zk4WUgFNPB/lVyufnRN4VKIWr5Eam++NtxkDcq1BVdzh65/SoBpjzPsDG7sxlWBjN6YSbOzGVEJXBbqIaN2DWgkwLGSpYJiSPiz2KCGHxS8VVMIiowryUcf43pWQw32U4MJipBIDS6rJlNyrygQrEeh4jdTzKCnTXLKvOfdRYiAHEKn1UPfKz2isY7P4p+6DBTolIPN7VVLG+p3vjfgdY8z7Chu7MZVgYzemEmzsxlRC1yPoOJKLxR4VocRZXSoTjMtSqYgxFldKSlcpEY0jptT+7Co6jo+VloFiWBBS53BklYqEY2FNCUsKFt9KzlNrzRF0JZGRKlqR3yGeH5CvUckedkAukqn74Kw7JULPnTu30VZiIGf0qXe4bZ95C3TGGBu7MbVgYzemErrqsx8/fjzLKisJSGC/RPnDHLShfBf260uq2SjY11VBJcpvYx9dzZHvX5XWZt9S+ZolfXhsFVRTUm675HmoPQPY/1X+OD8z9XxY51GaDs9Z3StfCyjTedhnVwFEnOWmSnvztdTWX6y9cKUa++zGGBu7MbVgYzemEmzsxlRCVwW6EydOyEye4SjhhMUllVHGlATDKPGJxTYV6NE27kjXP1WiWYmIx3MqyRZT66Guz4KcChDh67/66qtZn5KgGhb2SgRLtfZ8H+paapdhzowsKXm1e/furA+XeL722muzPgsWLGi0lRjI4/CecUosHsKf7MZUgo3dmEqwsRtTCV3f/on9KW4rn4z9LVVemX1EFZxT4sey36p8Ow6YUUElJVVoSvx6Ba+RSvzgsZVf3TYuoNdRJSsxfD3l627evLnRVqWcS/aQL0mE4XFKEqWA3G9WgVjc58iRI1mfhx9+uNFWPjvfv/K/OYBn3759jba6ryH8yW5MJdjYjakEG7sxlWBjN6YSul6phjOLWJRQYldJMApnQylhiYUbJfbw/Er2DFeCoQpGYWFLCWKMmmNbtR8gr1Sj1qwkYEhdn8U3lWXG1583b17WZ9u2bY32a6+9lvWZMWNGo61Eq7FU91HPVQXVcAabEuhYfFTP9dlnn220t2zZkvW54YYbGm31XC+99NJG++mnn260nfVmjLGxG1MLrcYeEWdGxK8j4tmI2BwR3+gcnxoRGyJioPP1/NM/XWPMWCnx2d8CsDKl9GZETALwy4h4EMCtAB5KKa2JiNUAVgO4e7SBjh07liVEcAUP5XOw36gSBNhHbNtmSo0L5H69CtDgBA5VhaXEjyypMKP8tpLqsnz/JcE6JYFIak5KZympQHThhRe2XovXerSgkZHmp85TVWnUe8XH1Hns66sAJtZHfvzjH2d9VqxY0Wjz3vBAvo68ZdRowVOtb2QaZOiOJ3X+JQC3AFjXOb4OwGfbxjLGjB9FPntE9EXEMwAOAdiQUvoVgFkppf0A0Pma/xgyxvQMRcaeUno7pXQlgLkAro6IpaUXiIg7ImJjRGwcLdfWGHN6OSk1PqX0OoBHAdwI4GBE9ANA5+uhEc5Zm1JallJaVpKMYYw5PbQKdBExA8CxlNLrEXEWgE8A+C8A6wHcBmBN5+v9bWOpSjUcaKKEJP4hoUSSkiASHkcJSyx2qWtxGWAVVFIiWimBkFFiE8+xJDhI9eG1V2KkqizEv6FxeXAAOHjwYKOtxDdeR67UAuRrW7L1l7oWC2QqoEgd47HVerD4p0REFhpffPHFrM+BAwcabZUFeP75zT96scA9WkZiiRrfD2BdRPRh8DeBH6aUHoiIJwD8MCJuB7AHwBcKxjLGjBOtxp5Seg7AVeL4EQA35GcYY3oRR9AZUwldTYSZOHEipk2b1jjGgQMlW+kqP5aDSFRQDY9TspWQ2sqHfX81Z5UMwWMr347nqHx/vjd1LQ5eUgEj7P+pcVTiB1eYee6557I+fG8qqIZ95LFWjuVjJc9erb3y9dlnLwnYUX1Y51CaEvvsaosovtaiRYsa7dF8dn+yG1MJNnZjKsHGbkwl2NiNqYSuCnSTJk3KBDkWoEpEGiWusACkBCHe/1sJdBw0MdbMNCWUcD81R+6jBCk+T12ft8gqyRZTASNqj3DOtOIAGnVMBcMsXLiwdU48b7WuvEYqOIjFNyXGqTUqCZgp2bKLr6eEVxYDVXg5i5pTpkxptF2pxhhjYzemFmzsxlRCV332CRMmZAkrJVsJsR+i/B0+pvwm9tmVf8NVP5XfxD670hnUsZLqNRywo3z2kgANXlcOoFHnlSSQAMBll13WaO/Zsyfrc/jw4UZb+exclbekSlHJWpcEx6g+JVV5xppgxc9j7ty5WZ+SKsr87FmfGK0ikT/ZjakEG7sxlWBjN6YSbOzGVEJXBbq+vr5MJGPhomRbHhWMwuKOCn7gLCJVhYYFGSUI8RxLAm8Uqk/J9blPSSlplQnG4o4S45RAyCWgly1blvXhbDn1PDjwpyQzTolfHAyktnHiZ62efUlVHrXWPG8l6pYI07zVlSrjxvuzc5DaaNWP/MluTCXY2I2pBBu7MZXQ9Uo1s2fPbhxjn7QkGUEFaIwl+IG32gXy5BgVnMO+nfJ1lb/FPrLyo0u0h5IKuHwfytdl/075uuoYz0lVQeWqp2qcEg2FK+yodeXnqKoLlfj1qpoPP7OSwB/l+7OPzoFJAHDJJZc02moLa9Y5HFRjjMmwsRtTCTZ2YyrBxm5MJYx7UA2LKUq0YkGuJPhCUdKHBY4jR45kfThoQglLSkTkeauAGb7/krLZKqOtpOINi1RKaFNrzQEiqg9XoVFrX1I9ht8PJTSysKbEN75+STajup56Hnw9Jfx++MMfbrRXrVqV9WFRVc2Rt5EqCd4awp/sxlSCjd2YSrCxG1MJNnZjKqHrZanaothKhK2SvbSUaMR9lNjDgsf27duzPpydNGfOnKyPKlPN96bEFc4W4z3bAB2hxXD0lRKNWFhk4Q0A+vv7s2O/+MUvGu3du3dnffj+lfBa8ly5lDXvFQjkwp4SA/nZq8hENUc+pqLzduzY0WgrwZTLUF100UVZH35mSmjkUl4s8jqCzhhjYzemFmzsxlRCV332lFKrj6G27mG/qaRaiPJd2JdTfj1rCgMDA1kf9hE/8pGPZH1Ktn9S98FzVOvB47z00ktZH/btlI/KPuLzzz+f9VG+/t69exttDvRQczx69GjWh9dIrQfrIUuWLMn6MCoYhZ+ZWo+S0s0qE421D6XhXH755Y222nud11VpSvzO8jZbSvcYwp/sxlSCjd2YSig29ojoi4jfRMQDnfbUiNgQEQOdr/nfG4wxPcPJfLLfCWDrsPZqAA+llBYDeKjTNsb0KEUCXUTMBXAzgP8E8C+dw7cAWNH5/zoAjwK4e7RxJkyYkIkyLIqUlDw+55xz5NijtYFcbFIiGgcyqECTxx9/vNG+6qqrsj6f/vSns2OceaYEIb7/RYsWZX04YEWJViyEqiAfDhDZt29f1ufAgQPZMRaOuLwxAOzatat1HBa21BxZyFJrxveqsglZ+CwZR81RiaFcam3BggVZn5tuuik71jZHtR48R157JegOUfrJ/i0AXwcw/EqzUkr7AaDzdaY4zxjTI7Qae0R8CsChlNKmsVwgIu6IiI0RsZFDQY0x3aPkk/06AJ+JiF0AfgBgZUTcC+BgRPQDQOfrIXVySmltSmlZSmkZV8Y0xnSPVp89pXQPgHsAICJWAPjXlNKXI+K/AdwGYE3n6/1tY0VE5u+x31zisyt/nMdVvgv7X6p6S4nfxP7efffdl/VZvHhxduziiy9utFXSD9+HSnrh+1D3WlJNh6+l9AmlGfD1Sspmq7HVnBh+P1QwDF9L+d78zEpKlgPAtm3bWufIgSyf/OQnsz68ZZZKHuL7UMFKrCnxO3y6tn9aA2BVRAwAWNVpG2N6lJMKl00pPYpB1R0ppSMAbjj1UzLGnA4cQWdMJdjYjamErma9RUQmuHBbCQwcDFNSglmJViy+cWYYkAeaqCwnzjziaioAcO+992bH7rrrrka7JGhC3SsfKylvzNlRQC6QqeAYNXZJhRkeW2Wi8X2oLER+rkpE43GUqFki4qmqRLxuSkCeP39+o/3Rj34068PvEQcdAXmFmylTpmR9VKWcUvzJbkwl2NiNqQQbuzGV0FWfHch9JfaBVHWOtjGA3N8r2decExiAvKIKVw8BgAsuuKDRVltEPfXUU9mx++9vxh195Stfyfpwko/SMPhYSYCIqlTKfqwK8lFj8/VVkBOPpfx6PqaSU3iOapySakdcqebw4cNZH+WzM0rnufnmm1v7bN68uXVsTvpRz4x9f/bhXanGGGNjN6YWbOzGVIKN3ZhK6KpAd/z48SxwgwWFkv3AVdAEb8k0mlAxhMrWKsnCY/FJBceojK4HH3yw0VYVTVjs4f3sgXyNlCDE6cQqQKOkvLIqJc0BMmq7JV5blfXGqHFYMC3Zn10F8HAthZ07d2Z91LNmeBsnALjssssabSXGsdB79dVXZ314zUq2/uK2Eq+H8Ce7MZVgYzemEmzsxlRCV332EydOZP4V+8jKH2cfWSVsKF+KYZ9U+fUzZzbrZip/nP1YToRQfYDcJ/z5z3+e9eGxli9fnvUpqcrD81ZaCKN8dtZCVD/la/NzVhoGn6eCk3gdVR/269V8eBz1nikNh9dx2bJlrWNv3bo168NbQqlrcYCMuld+1rzODqoxxtjYjakFG7sxlWBjN6YSuh5Uw4EC/f39jbYSiVigU+IXi3YqiIRFKiW+8XzUVlMsAM2aNSvr8/LLL2fHeCwVIPLII4802ioYhff6VnNkVGZaSRlilYnGqGfG96b6lOx9zttGHToktydoUDJnleGn3ite60svvTTrs2XLlkZbZatNnz690eaAJiAX5FRVGhbgeA1Hu3d/shtTCTZ2YyrBxm5MJXTVZ3/rrbewY8eOxjFO4lDBBiXVQ9m3U1VoeGzlW3FyysKFC7M+HDShfF32/YE8GUMlsLDP9cQTT2R92I9XW02xrqD8+pKKNyXVY9R57JMq/5P9b1UBlwNmlM7C66i2nmaUz64SYXjbJuXX830oDYd9a+Wz870qW+B3iNfsVGzZbIx5j2NjN6YSbOzGVIKN3ZhKiJIAhFN2sYjDAHYDmA4gL8PR+7wX5+05d4demfOFKaU8VRFdNvZ3LhqxMaWU5wr2OO/FeXvO3eG9MGf/Gm9MJdjYjamE8TL2teN03XfLe3HennN36Pk5j4vPbozpPv413phK6LqxR8SNEbEtIrZHxOpuX7+EiPhORByKiBeGHZsaERsiYqDz9fzxnCMTEfMi4pGI2BoRmyPizs7xnp13RJwZEb+OiGc7c/5G53jPznmIiOiLiN9ExAOdds/PuavGHhF9AP4XwN8BWALgSxGxpJtzKOS7AG6kY6sBPJRSWgzgoU67lzgO4K6U0mUAlgP4p87a9vK83wKwMqV0BYArAdwYEcvR23Me4k4AwzOien/OKaWu/QPwtwB+Nqx9D4B7ujmHk5jrAgAvDGtvA9Df+X8/gG3jPceW+d8PYNV7Zd4AzgbwNIBren3OAOZi0KBXAnjgvfJ+dPvX+DkAhtdr2ts59l5gVkppPwB0vs5s6T9uRMQCAFcB+BV6fN6dX4efAXAIwIaUUs/PGcC3AHwdwPDc616fc9eNPU/8BvzngFNIRJwL4D4AX00p5UnkPUZK6e2U0pUY/LS8OiKWjvOURiUiPgXgUEpp03jP5WTptrHvBTBvWHsugPZKA73BwYjoB4DO1/bKh10mIiZh0NC/l1L6Uedwz88bAFJKrwN4FINaSS/P+ToAn4mIXQB+AGBlRNyL3p4zgO4b+1MAFkfEwoiYDOCLANZ3eQ5jZT2A2zr/vw2DPnHPEINlZ74NYGtK6ZvDvtWz846IGRExpfP/swB8AsCL6OE5p5TuSSnNTSktwOD7+3BK6cvo4Tm/wziIGzcB+C2AlwD8+3iLFiPM8fsA9gM4hsHfRm4HMA2DosxA5+vU8Z4nzfl6DLpEzwF4pvPvpl6eN4C/AfCbzpxfAPAfneM9O2ea/wr8VaDr+Tk7gs6YSnAEnTGVYGM3phJs7MZUgo3dmEqwsRtTCTZ2YyrBxm5MJdjYjamE/wd94aXzKgANRwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "    \n",
    "print(train_images.shape)\n",
    "imshow(torchvision.utils.make_grid(train_images,nrow=4))\n",
    "#print(train_images.shape)\n",
    "#print((torchvision.utils.make_grid(train_images)).shape)\n",
    "#print(\" \".join(\"%s\"%classes[train_labels[j]] for j in range(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        # BatchNorm에 bias가 포함되어 있으므로, conv2d는 bias=False로 설정합니다.\n",
    "        self.residual_function = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels * BasicBlock.expansion, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels * BasicBlock.expansion),\n",
    "        )\n",
    "\n",
    "        # identity mapping, input과 output의 feature map size, filter 수가 동일한 경우 사용.\n",
    "        self.shortcut = nn.Sequential()\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # projection mapping using 1x1conv\n",
    "        if stride != 1 or in_channels != BasicBlock.expansion * out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels * BasicBlock.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * BasicBlock.expansion)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.residual_function(x) + self.shortcut(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class BottleNeck(nn.Module):\n",
    "    expansion = 4\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.residual_function = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels * BottleNeck.expansion, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels * BottleNeck.expansion),\n",
    "        )\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        if stride != 1 or in_channels != out_channels * BottleNeck.expansion:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels*BottleNeck.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels*BottleNeck.expansion)\n",
    "            )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.residual_function(x) + self.shortcut(x)\n",
    "        x = self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_block, num_classes=10, init_weights=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels=64\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "\n",
    "        self.conv2_x = self._make_layer(block, 64, num_block[0], 1)\n",
    "        self.conv3_x = self._make_layer(block, 128, num_block[1], 2)\n",
    "        self.conv4_x = self._make_layer(block, 256, num_block[2], 2)\n",
    "        self.conv5_x = self._make_layer(block, 512, num_block[3], 2)\n",
    "\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        # weights inittialization\n",
    "        if init_weights:\n",
    "            self._initialize_weights()\n",
    "\n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_channels, out_channels, stride))\n",
    "            self.in_channels = out_channels * block.expansion\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self,x):\n",
    "        output = self.conv1(x)\n",
    "        output = self.conv2_x(output)\n",
    "        x = self.conv3_x(output)\n",
    "        x = self.conv4_x(x)\n",
    "        x = self.conv5_x(x)\n",
    "        x = self.avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    # define weight initialization function\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "def resnet18():\n",
    "    return ResNet(BasicBlock, [2,2,2,2])\n",
    "\n",
    "def resnet34():\n",
    "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
    "\n",
    "def resnet50():\n",
    "    return ResNet(BottleNeck, [3,4,6,3])\n",
    "\n",
    "def resnet101():\n",
    "    return ResNet(BottleNeck, [3, 4, 23, 3])\n",
    "\n",
    "def resnet152():\n",
    "    return ResNet(BottleNeck, [3, 8, 36, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Deep_Emotion(nn.Module):\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Deep_Emotion class contains the network architecture.\n",
    "        '''\n",
    "        super(Deep_Emotion,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1,10,3)\n",
    "        self.conv2 = nn.Conv2d(10,10,3)\n",
    "        self.pool2 = nn.MaxPool2d(2,2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(10,10,3)\n",
    "        self.conv4 = nn.Conv2d(10,10,3)\n",
    "        self.pool4 = nn.MaxPool2d(2,2)\n",
    "\n",
    "        self.Resnet34 = resnet34()\n",
    "        self.norm = nn.BatchNorm2d(10)\n",
    "\n",
    "        self.fc1 = nn.Linear(10,50)\n",
    "        self.fc2 = nn.Linear(50,7)\n",
    "\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        self.localization = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=7),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(8, 10, kernel_size=5),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "        self.fc_loc = nn.Sequential(\n",
    "            nn.Linear(640,32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(32, 2*3)\n",
    "        )\n",
    "        self.fc_loc[2].weight.data.zero_()\n",
    "        self.fc_loc[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))\n",
    "\n",
    "    def stn(self, x):\n",
    "        xs = self.localization(x)\n",
    "        xs = xs.view(-1, 640)\n",
    "        theta = self.fc_loc(xs)\n",
    "        theta = theta.view(-1, 2, 3)\n",
    "        grid = F.affine_grid(theta, x.size())\n",
    "        x = F.grid_sample(x, grid,align_corners=True)\n",
    "   \n",
    "        return x\n",
    "\n",
    "    def forward(self,input):\n",
    "        out = self.stn(input)\n",
    "        out = self.Resnet34(out)\n",
    "        out = F.dropout(out)\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 7])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1315\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:3448: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\"Default grid_sample and affine_grid behavior has changed \"\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Deep_Emotion().to(device)\n",
    "x = torch.randn(3, 1, 48, 48).to(device)\n",
    "output = model(x)\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 8, 42, 42]             400\n",
      "         MaxPool2d-2            [-1, 8, 21, 21]               0\n",
      "              ReLU-3            [-1, 8, 21, 21]               0\n",
      "            Conv2d-4           [-1, 10, 17, 17]           2,010\n",
      "         MaxPool2d-5             [-1, 10, 8, 8]               0\n",
      "              ReLU-6             [-1, 10, 8, 8]               0\n",
      "            Linear-7                   [-1, 32]          20,512\n",
      "              ReLU-8                   [-1, 32]               0\n",
      "            Linear-9                    [-1, 6]             198\n",
      "           Conv2d-10           [-1, 64, 24, 24]           3,136\n",
      "      BatchNorm2d-11           [-1, 64, 24, 24]             128\n",
      "             ReLU-12           [-1, 64, 24, 24]               0\n",
      "        MaxPool2d-13           [-1, 64, 12, 12]               0\n",
      "           Conv2d-14           [-1, 64, 12, 12]          36,864\n",
      "      BatchNorm2d-15           [-1, 64, 12, 12]             128\n",
      "             ReLU-16           [-1, 64, 12, 12]               0\n",
      "           Conv2d-17           [-1, 64, 12, 12]          36,864\n",
      "      BatchNorm2d-18           [-1, 64, 12, 12]             128\n",
      "             ReLU-19           [-1, 64, 12, 12]               0\n",
      "       BasicBlock-20           [-1, 64, 12, 12]               0\n",
      "           Conv2d-21           [-1, 64, 12, 12]          36,864\n",
      "      BatchNorm2d-22           [-1, 64, 12, 12]             128\n",
      "             ReLU-23           [-1, 64, 12, 12]               0\n",
      "           Conv2d-24           [-1, 64, 12, 12]          36,864\n",
      "      BatchNorm2d-25           [-1, 64, 12, 12]             128\n",
      "             ReLU-26           [-1, 64, 12, 12]               0\n",
      "       BasicBlock-27           [-1, 64, 12, 12]               0\n",
      "           Conv2d-28           [-1, 64, 12, 12]          36,864\n",
      "      BatchNorm2d-29           [-1, 64, 12, 12]             128\n",
      "             ReLU-30           [-1, 64, 12, 12]               0\n",
      "           Conv2d-31           [-1, 64, 12, 12]          36,864\n",
      "      BatchNorm2d-32           [-1, 64, 12, 12]             128\n",
      "             ReLU-33           [-1, 64, 12, 12]               0\n",
      "       BasicBlock-34           [-1, 64, 12, 12]               0\n",
      "           Conv2d-35            [-1, 128, 6, 6]          73,728\n",
      "      BatchNorm2d-36            [-1, 128, 6, 6]             256\n",
      "             ReLU-37            [-1, 128, 6, 6]               0\n",
      "           Conv2d-38            [-1, 128, 6, 6]         147,456\n",
      "      BatchNorm2d-39            [-1, 128, 6, 6]             256\n",
      "           Conv2d-40            [-1, 128, 6, 6]           8,192\n",
      "      BatchNorm2d-41            [-1, 128, 6, 6]             256\n",
      "             ReLU-42            [-1, 128, 6, 6]               0\n",
      "       BasicBlock-43            [-1, 128, 6, 6]               0\n",
      "           Conv2d-44            [-1, 128, 6, 6]         147,456\n",
      "      BatchNorm2d-45            [-1, 128, 6, 6]             256\n",
      "             ReLU-46            [-1, 128, 6, 6]               0\n",
      "           Conv2d-47            [-1, 128, 6, 6]         147,456\n",
      "      BatchNorm2d-48            [-1, 128, 6, 6]             256\n",
      "             ReLU-49            [-1, 128, 6, 6]               0\n",
      "       BasicBlock-50            [-1, 128, 6, 6]               0\n",
      "           Conv2d-51            [-1, 128, 6, 6]         147,456\n",
      "      BatchNorm2d-52            [-1, 128, 6, 6]             256\n",
      "             ReLU-53            [-1, 128, 6, 6]               0\n",
      "           Conv2d-54            [-1, 128, 6, 6]         147,456\n",
      "      BatchNorm2d-55            [-1, 128, 6, 6]             256\n",
      "             ReLU-56            [-1, 128, 6, 6]               0\n",
      "       BasicBlock-57            [-1, 128, 6, 6]               0\n",
      "           Conv2d-58            [-1, 128, 6, 6]         147,456\n",
      "      BatchNorm2d-59            [-1, 128, 6, 6]             256\n",
      "             ReLU-60            [-1, 128, 6, 6]               0\n",
      "           Conv2d-61            [-1, 128, 6, 6]         147,456\n",
      "      BatchNorm2d-62            [-1, 128, 6, 6]             256\n",
      "             ReLU-63            [-1, 128, 6, 6]               0\n",
      "       BasicBlock-64            [-1, 128, 6, 6]               0\n",
      "           Conv2d-65            [-1, 256, 3, 3]         294,912\n",
      "      BatchNorm2d-66            [-1, 256, 3, 3]             512\n",
      "             ReLU-67            [-1, 256, 3, 3]               0\n",
      "           Conv2d-68            [-1, 256, 3, 3]         589,824\n",
      "      BatchNorm2d-69            [-1, 256, 3, 3]             512\n",
      "           Conv2d-70            [-1, 256, 3, 3]          32,768\n",
      "      BatchNorm2d-71            [-1, 256, 3, 3]             512\n",
      "             ReLU-72            [-1, 256, 3, 3]               0\n",
      "       BasicBlock-73            [-1, 256, 3, 3]               0\n",
      "           Conv2d-74            [-1, 256, 3, 3]         589,824\n",
      "      BatchNorm2d-75            [-1, 256, 3, 3]             512\n",
      "             ReLU-76            [-1, 256, 3, 3]               0\n",
      "           Conv2d-77            [-1, 256, 3, 3]         589,824\n",
      "      BatchNorm2d-78            [-1, 256, 3, 3]             512\n",
      "             ReLU-79            [-1, 256, 3, 3]               0\n",
      "       BasicBlock-80            [-1, 256, 3, 3]               0\n",
      "           Conv2d-81            [-1, 256, 3, 3]         589,824\n",
      "      BatchNorm2d-82            [-1, 256, 3, 3]             512\n",
      "             ReLU-83            [-1, 256, 3, 3]               0\n",
      "           Conv2d-84            [-1, 256, 3, 3]         589,824\n",
      "      BatchNorm2d-85            [-1, 256, 3, 3]             512\n",
      "             ReLU-86            [-1, 256, 3, 3]               0\n",
      "       BasicBlock-87            [-1, 256, 3, 3]               0\n",
      "           Conv2d-88            [-1, 256, 3, 3]         589,824\n",
      "      BatchNorm2d-89            [-1, 256, 3, 3]             512\n",
      "             ReLU-90            [-1, 256, 3, 3]               0\n",
      "           Conv2d-91            [-1, 256, 3, 3]         589,824\n",
      "      BatchNorm2d-92            [-1, 256, 3, 3]             512\n",
      "             ReLU-93            [-1, 256, 3, 3]               0\n",
      "       BasicBlock-94            [-1, 256, 3, 3]               0\n",
      "           Conv2d-95            [-1, 256, 3, 3]         589,824\n",
      "      BatchNorm2d-96            [-1, 256, 3, 3]             512\n",
      "             ReLU-97            [-1, 256, 3, 3]               0\n",
      "           Conv2d-98            [-1, 256, 3, 3]         589,824\n",
      "      BatchNorm2d-99            [-1, 256, 3, 3]             512\n",
      "            ReLU-100            [-1, 256, 3, 3]               0\n",
      "      BasicBlock-101            [-1, 256, 3, 3]               0\n",
      "          Conv2d-102            [-1, 256, 3, 3]         589,824\n",
      "     BatchNorm2d-103            [-1, 256, 3, 3]             512\n",
      "            ReLU-104            [-1, 256, 3, 3]               0\n",
      "          Conv2d-105            [-1, 256, 3, 3]         589,824\n",
      "     BatchNorm2d-106            [-1, 256, 3, 3]             512\n",
      "            ReLU-107            [-1, 256, 3, 3]               0\n",
      "      BasicBlock-108            [-1, 256, 3, 3]               0\n",
      "          Conv2d-109            [-1, 512, 2, 2]       1,179,648\n",
      "     BatchNorm2d-110            [-1, 512, 2, 2]           1,024\n",
      "            ReLU-111            [-1, 512, 2, 2]               0\n",
      "          Conv2d-112            [-1, 512, 2, 2]       2,359,296\n",
      "     BatchNorm2d-113            [-1, 512, 2, 2]           1,024\n",
      "          Conv2d-114            [-1, 512, 2, 2]         131,072\n",
      "     BatchNorm2d-115            [-1, 512, 2, 2]           1,024\n",
      "            ReLU-116            [-1, 512, 2, 2]               0\n",
      "      BasicBlock-117            [-1, 512, 2, 2]               0\n",
      "          Conv2d-118            [-1, 512, 2, 2]       2,359,296\n",
      "     BatchNorm2d-119            [-1, 512, 2, 2]           1,024\n",
      "            ReLU-120            [-1, 512, 2, 2]               0\n",
      "          Conv2d-121            [-1, 512, 2, 2]       2,359,296\n",
      "     BatchNorm2d-122            [-1, 512, 2, 2]           1,024\n",
      "            ReLU-123            [-1, 512, 2, 2]               0\n",
      "      BasicBlock-124            [-1, 512, 2, 2]               0\n",
      "          Conv2d-125            [-1, 512, 2, 2]       2,359,296\n",
      "     BatchNorm2d-126            [-1, 512, 2, 2]           1,024\n",
      "            ReLU-127            [-1, 512, 2, 2]               0\n",
      "          Conv2d-128            [-1, 512, 2, 2]       2,359,296\n",
      "     BatchNorm2d-129            [-1, 512, 2, 2]           1,024\n",
      "            ReLU-130            [-1, 512, 2, 2]               0\n",
      "      BasicBlock-131            [-1, 512, 2, 2]               0\n",
      "AdaptiveAvgPool2d-132            [-1, 512, 1, 1]               0\n",
      "          Linear-133                   [-1, 10]           5,130\n",
      "          ResNet-134                   [-1, 10]               0\n",
      "          Linear-135                   [-1, 50]             550\n",
      "          Linear-136                    [-1, 7]             357\n",
      "================================================================\n",
      "Total params: 21,307,557\n",
      "Trainable params: 21,307,557\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 4.78\n",
      "Params size (MB): 81.28\n",
      "Estimated Total Size (MB): 86.07\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, (1, 48, 48), device=device.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss(reduction='sum')\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "lr_scheduler = ReduceLROnPlateau(opt, mode='min', factor=0.1, patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get current lr\n",
    "def get_lr(opt):\n",
    "    for param_group in opt.param_groups:\n",
    "        return param_group['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate metric per mini-batch\n",
    "def metric_batch(output, target):\n",
    "    pred = output.argmax(1, keepdim=True)\n",
    "    corrects = pred.eq(target.view_as(pred)).sum().item()\n",
    "    return corrects\n",
    "\n",
    "\n",
    "# function to calculate loss per mini-batch\n",
    "def loss_batch(loss_func, output, target, opt=None):\n",
    "    loss = loss_func(output, target)\n",
    "    metric_b = metric_batch(output, target)\n",
    "\n",
    "    if opt is not None:\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "    return loss.item(), metric_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate loss and metric per epoch\n",
    "def loss_epoch(model, loss_func, dataset_dl, sanity_check=False, opt=None):\n",
    "    running_loss = 0.0\n",
    "    running_metric = 0.0\n",
    "    len_data = len(dataset_dl.dataset)\n",
    "\n",
    "    for xb, yb in dataset_dl:\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "        output = model(xb)\n",
    "\n",
    "        loss_b, metric_b = loss_batch(loss_func, output, yb, opt)\n",
    "\n",
    "        running_loss += loss_b\n",
    "        \n",
    "        if metric_b is not None:\n",
    "            running_metric += metric_b\n",
    "        \n",
    "        if sanity_check is True:\n",
    "            break\n",
    "\n",
    "    loss = running_loss / len_data\n",
    "    metric = running_metric / len_data\n",
    "\n",
    "    return loss, metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to start training\n",
    "def train_val(model, params):\n",
    "    num_epochs=params['num_epochs']\n",
    "    loss_func=params[\"loss_func\"]\n",
    "    opt=params[\"optimizer\"]\n",
    "    train_dl=params[\"train_dl\"]\n",
    "    val_dl=params[\"val_dl\"]\n",
    "    sanity_check=params[\"sanity_check\"]\n",
    "    lr_scheduler=params[\"lr_scheduler\"]\n",
    "    path2weights=params[\"path2weights\"]\n",
    "\n",
    "    loss_history = {'train': [], 'val': []}\n",
    "    metric_history = {'train': [], 'val': []}\n",
    "\n",
    "    # # GPU out of memoty error\n",
    "    # best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        current_lr = get_lr(opt)\n",
    "        print('Epoch {}/{}, current lr={}'.format(epoch, num_epochs-1, current_lr))\n",
    "\n",
    "        model.train()\n",
    "        train_loss, train_metric = loss_epoch(model, loss_func, train_dl, sanity_check, opt)\n",
    "        loss_history['train'].append(train_loss)\n",
    "        metric_history['train'].append(train_metric)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss, val_metric = loss_epoch(model, loss_func, val_dl, sanity_check)\n",
    "        loss_history['val'].append(val_loss)\n",
    "        metric_history['val'].append(val_metric)\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            # best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "            # torch.save(model.state_dict(), path2weights)\n",
    "            # print('Copied best model weights!')\n",
    "            print('Get best val_loss')\n",
    "\n",
    "        lr_scheduler.step(val_loss)\n",
    "\n",
    "        print('train loss: %.6f, val loss: %.6f, accuracy: %.2f, time: %.4f min' %(train_loss, val_loss, 100*val_metric, (time.time()-start_time)/60))\n",
    "        print('-'*10)\n",
    "\n",
    "    # model.load_state_dict(best_model_wts)\n",
    "\n",
    "    return model, loss_history, metric_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definc the training parameters\n",
    "params_train = {\n",
    "    'num_epochs':50,\n",
    "    'optimizer':opt,\n",
    "    'loss_func':loss_func,\n",
    "    'train_dl':trainloader,\n",
    "    'val_dl':testloader,\n",
    "    'sanity_check':False,\n",
    "    'lr_scheduler':lr_scheduler,\n",
    "    'path2weights':'./models/weights.pt',\n",
    "}\n",
    "\n",
    "# create the directory that stores weights.pt\n",
    "def createFolder(directory):\n",
    "    try:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "    except OSerror:\n",
    "        print('Error')\n",
    "createFolder('./models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/49, current lr=0.005\n",
      "Get best val_loss\n",
      "train loss: 1.817949, val loss: 1.814630, accuracy: 24.71, time: 14.8424 min\n",
      "----------\n",
      "Epoch 1/49, current lr=0.005\n",
      "train loss: 1.812698, val loss: 1.815536, accuracy: 24.71, time: 29.4282 min\n",
      "----------\n",
      "Epoch 2/49, current lr=0.005\n",
      "Get best val_loss\n",
      "train loss: 1.812733, val loss: 1.814591, accuracy: 24.71, time: 43.9025 min\n",
      "----------\n",
      "Epoch 3/49, current lr=0.005\n",
      "train loss: 1.812359, val loss: 1.815517, accuracy: 24.71, time: 58.7090 min\n",
      "----------\n",
      "Epoch 4/49, current lr=0.005\n",
      "train loss: 1.812763, val loss: 1.814689, accuracy: 24.71, time: 73.3636 min\n",
      "----------\n",
      "Epoch 5/49, current lr=0.005\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16180/3749987318.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_hist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric_hist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_val\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16180/774778175.py\u001b[0m in \u001b[0;36mtrain_val\u001b[1;34m(model, params)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_metric\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msanity_check\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[0mloss_history\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mmetric_history\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_metric\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16180/1697359548.py\u001b[0m in \u001b[0;36mloss_epoch\u001b[1;34m(model, loss_func, dataset_dl, sanity_check, opt)\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mloss_b\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric_b\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss_b\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16180/7985490.py\u001b[0m in \u001b[0;36mloss_batch\u001b[1;34m(loss_func, output, target, opt)\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric_b\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m             \u001b[0mbeta1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'betas'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m             F.adam(params_with_grad,\n\u001b[0m\u001b[0;32m    109\u001b[0m                    \u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m                    \u001b[0mexp_avgs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\functional.py\u001b[0m in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[1;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[0mexp_avg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m             \u001b[1;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model, loss_hist, metric_hist = train_val(model, params_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_model(model, num_images=6):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(trainloader['val']):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            for j in range(inputs.size()[0]):\n",
    "                images_so_far += 1\n",
    "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
    "                ax.axis('off')\n",
    "                ax.set_title('predicted: {}'.format(classes[preds[j]]))\n",
    "                imshow(inputs.cpu().data[j])\n",
    "\n",
    "                if images_so_far == num_images:\n",
    "                    model.train(mode=was_training)\n",
    "                    return\n",
    "        model.train(mode=was_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2b06eea86720a957e96073e5e3b3d48d33ced1ae9c666ea436625d157f43d685"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
