{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.12 ('-WIL-Expression-Recognition-Study')' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: 'conda install -n -WIL-Expression-Recognition-Study ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBClassifier\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "\n",
    "torch.manual_seed(2020) \n",
    "np.random.seed(2020)\n",
    "random.seed(2020)\n",
    "\n",
    "embedding_dims =7\n",
    "batch_size = 18\n",
    "epochs =50\n",
    "\n",
    "train_df = pd.read_csv(\"C:/Users/1315/Desktop/data/ck_train.csv\")\n",
    "test_df = pd.read_csv(\"C:/Users/1315/Desktop/data/ck_val.csv\")\n",
    "\n",
    "print(train_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.12 ('-WIL-Expression-Recognition-Study')' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: 'conda install -n -WIL-Expression-Recognition-Study ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "class ImageData(Dataset):\n",
    "        \n",
    "\n",
    "    def __init__(self, df, train =True, transform =None):\n",
    "        self.is_train =train\n",
    "        self.transform = transform \n",
    "        self.to_pil = transforms.ToPILImage() ## torch에서 이미지 처리 \n",
    "\n",
    "        \n",
    "        if self.is_train:\n",
    "\n",
    "            self.images = df['pixels']\n",
    "            self.labels = df['emotion']\n",
    "            self.index = df.index.values\n",
    "        \n",
    "        else:\n",
    "            self.images = df['pixels']\n",
    "            self.labels = df['emotion']\n",
    "            self.index = df.index.values \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, item): \n",
    "\n",
    "        imgarray_str = self.images[item].split(' ')\n",
    "        imgarray = np.asarray(imgarray_str,dtype=np.uint8).reshape(48,48,1)\n",
    "        anchor_img = imgarray\n",
    "        \n",
    "        if self.is_train:\n",
    "            anchor_label = self.labels[item]\n",
    "            \n",
    "            ## 해당 anchor가 아닌 것들중에서 Label 같은 것들의 index를 가지고 옮\n",
    "            positive_list = self.index[self.index != item][self.labels[self.index!=item]==anchor_label]\n",
    "            \n",
    "            positive_item = random.choice(positive_list)\n",
    "            imgarray_str = self.images[positive_item].split(' ')\n",
    "            imgarray = np.asarray(imgarray_str,dtype=np.uint8).reshape(48,48,1) \n",
    "            positive_img =imgarray\n",
    "            \n",
    "            ## 해당 anchor가 아닌 것들중에서 Label 다른 것들의 index를 가지고 옮\n",
    "            negative_list = self.index[self.index != item][self.labels[self.index!=item]!=anchor_label]\n",
    "            \n",
    "            nagative_item = random.choice(negative_list)\n",
    "            imgarray_str = self.images[nagative_item].split(' ')\n",
    "            imgarray = np.asarray(imgarray_str,dtype=np.uint8).reshape(48,48,1) \n",
    "            negative_img = imgarray\n",
    "            \n",
    "            if self.transform:\n",
    "                anchor_img = self.transform(self.to_pil(anchor_img))\n",
    "                positive_img = self.transform(self.to_pil(positive_img))\n",
    "                negative_img = self.transform(self.to_pil(negative_img))\n",
    "            \n",
    "            return anchor_img, positive_img, negative_img, anchor_label\n",
    "        \n",
    "        else:\n",
    "            if self.transform:\n",
    "                anchor_img = self.transform(self.to_pil(anchor_img))\n",
    "            return anchor_img            \n",
    "            \n",
    "\n",
    "train_ds = ImageData(train_df, \n",
    "                 train=True,\n",
    "                 transform=transforms.Compose([\n",
    "                     transforms.ToTensor()\n",
    "                 ]))\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "\n",
    "test_ds = ImageData(test_df, train=False, transform=transforms.Compose([\n",
    "                     transforms.ToTensor()\n",
    "                 ]))\n",
    "test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.12 ('-WIL-Expression-Recognition-Study')' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: 'conda install -n -WIL-Expression-Recognition-Study ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "class TripletLoss(nn.Module):\n",
    "    def __init__(self, margin = 1.0):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        \n",
    "    def calc_euclidean(self, x1, x2):\n",
    "        return (x1-x2).pow(2).sum(1) \n",
    "    \n",
    "    def forward(self,  anchor: torch.Tensor, positive: torch.Tensor, negative: torch.Tensor):\n",
    "        \n",
    "        distance_positive = self.calc_euclidean(anchor, positive)\n",
    "        distance_negative = self.calc_euclidean(anchor, negative)\n",
    "        losses = torch.relu(distance_positive - distance_negative + self.margin)\n",
    "\n",
    "        return losses.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.12 ('-WIL-Expression-Recognition-Study')' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: 'conda install -n -WIL-Expression-Recognition-Study ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        # BatchNorm에 bias가 포함되어 있으므로, conv2d는 bias=False로 설정합니다.\n",
    "        self.residual_function = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels * BasicBlock.expansion, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels * BasicBlock.expansion),\n",
    "        )\n",
    "\n",
    "        # identity mapping, input과 output의 feature map size, filter 수가 동일한 경우 사용.\n",
    "        self.shortcut = nn.Sequential()\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # projection mapping using 1x1conv\n",
    "        if stride != 1 or in_channels != BasicBlock.expansion * out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels * BasicBlock.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * BasicBlock.expansion)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.residual_function(x) + self.shortcut(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class BottleNeck(nn.Module):\n",
    "    expansion = 4\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.residual_function = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels * BottleNeck.expansion, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels * BottleNeck.expansion),\n",
    "        )\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        if stride != 1 or in_channels != out_channels * BottleNeck.expansion:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels*BottleNeck.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels*BottleNeck.expansion)\n",
    "            )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.residual_function(x) + self.shortcut(x)\n",
    "        x = self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.12 ('-WIL-Expression-Recognition-Study')' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: 'conda install -n -WIL-Expression-Recognition-Study ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_block, num_classes=7, init_weights=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels=64\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "\n",
    "        self.conv2_x = self._make_layer(block, 64, num_block[0], 1)\n",
    "        self.conv3_x = self._make_layer(block, 128, num_block[1], 2)\n",
    "        self.conv4_x = self._make_layer(block, 256, num_block[2], 2)\n",
    "        self.conv5_x = self._make_layer(block, 512, num_block[3], 2)\n",
    "\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        # weights inittialization\n",
    "        if init_weights:\n",
    "            self._initialize_weights()\n",
    "\n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_channels, out_channels, stride))\n",
    "            self.in_channels = out_channels * block.expansion\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self,x):\n",
    "        output = self.conv1(x)\n",
    "        output = self.conv2_x(output)\n",
    "        x = self.conv3_x(output)\n",
    "        x = self.conv4_x(x)\n",
    "        x = self.conv5_x(x)\n",
    "        x = self.avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    # define weight initialization function\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "def resnet18():\n",
    "    return ResNet(BasicBlock, [2,2,2,2])\n",
    "\n",
    "def resnet34():\n",
    "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
    "\n",
    "def resnet50():\n",
    "    return ResNet(BottleNeck, [3,4,6,3])\n",
    "\n",
    "def resnet101():\n",
    "    return ResNet(BottleNeck, [3, 4, 23, 3])\n",
    "\n",
    "def resnet152():\n",
    "    return ResNet(BottleNeck, [3, 8, 36, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.12 ('-WIL-Expression-Recognition-Study')' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: 'conda install -n -WIL-Expression-Recognition-Study ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Deep_Emotion class contains the network architecture.\n",
    "        '''\n",
    "        super(Network,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1,10,3)\n",
    "        self.conv2 = nn.Conv2d(10,10,3)\n",
    "        self.pool2 = nn.MaxPool2d(2,2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(10,10,3)\n",
    "        self.conv4 = nn.Conv2d(10,10,3)\n",
    "        self.pool4 = nn.MaxPool2d(2,2)\n",
    "\n",
    "        self.Resnet18 = resnet18()\n",
    "        self.norm = nn.BatchNorm2d(10)\n",
    "\n",
    "        self.fc1 = nn.Linear(10,50)\n",
    "        self.fc2 = nn.Linear(50,7)\n",
    "\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(512, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(32, 512),\n",
    "            nn.Sigmoid()    \n",
    "        )\n",
    "\n",
    "        self.localization = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=7),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(8, 10, kernel_size=5),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "        self.fc_loc = nn.Sequential(\n",
    "            nn.Linear(640, 32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(32, 3 * 2)\n",
    "        )\n",
    "        self.fc_loc[2].weight.data.zero_()\n",
    "        self.fc_loc[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))\n",
    "\n",
    "    def stn(self, x):\n",
    "        xs = self.localization(x)\n",
    "        xs = F.dropout(xs)\n",
    "        xs = xs.view(-1, 640)\n",
    "        theta = self.fc_loc(xs)\n",
    "        theta = theta.view(-1, 2, 3)\n",
    "       \n",
    "        grid = F.affine_grid(theta, x.size())\n",
    "        x = F.grid_sample(x, grid)\n",
    "   \n",
    "        return x\n",
    "\n",
    "    def forward(self,input):\n",
    "        out = self.stn(input)\n",
    "        out = self.Resnet18(out)\n",
    "        # out = F.dropout(out)\n",
    "        # out = F.relu(self.fc1(out))\n",
    "        # out = self.fc2(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        torch.nn.init.kaiming_normal_(m.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.12 ('-WIL-Expression-Recognition-Study')' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: 'conda install -n -WIL-Expression-Recognition-Study ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "## Create instance\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Network().to(device)\n",
    "model.apply(init_weights)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# criterion = TripletLoss()\n",
    "criterion = nn.TripletMarginLoss(margin=1.0, p=2)\n",
    "    \n",
    "\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    running_loss = []\n",
    "    for step, (anchor_img, positive_img, negative_img, anchor_label) in enumerate(train_loader):\n",
    "        anchor_img, positive_img, negative_img, anchor_label = anchor_img.to(device), positive_img.to(device),negative_img.to(device), anchor_label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        anchor_out = model(anchor_img)\n",
    "        positive_out = model(positive_img)\n",
    "        negative_out = model(negative_img)\n",
    "        \n",
    "        loss = criterion(anchor_out, positive_out, negative_out)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "             \n",
    "        running_loss.append(loss.detach().cpu().numpy())\n",
    "    print(\"Epoch: {}/{} - Loss: {:.4f}\".format(epoch+1, epochs, np.mean(running_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.12 ('-WIL-Expression-Recognition-Study')' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: 'conda install -n -WIL-Expression-Recognition-Study ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "train_results =[]\n",
    "labels =[]\n",
    "classes=['Angry','Disgust','Fear','Happy','Sad','Surprise','Neutral']\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for img, _, _, label in train_loader:\n",
    "        img,label = img.to(device),label.to(device)\n",
    "        train_results.append(model(img).cpu().numpy())\n",
    "        labels.append(label.cpu())\n",
    "        \n",
    "train_results = np.concatenate(train_results)\n",
    "labels = np.concatenate(labels)\n",
    "train_results.shape\n",
    "\n",
    "## visualization\n",
    "plt.figure(figsize=(15, 10), facecolor=\"azure\")\n",
    "for label in np.unique(labels):\n",
    "    tmp = train_results[labels==label]\n",
    "    plt.scatter(tmp[:, 0], tmp[:, 1], label=classes[label])\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.12 ('-WIL-Expression-Recognition-Study')' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: 'conda install -n -WIL-Expression-Recognition-Study ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "tree = XGBClassifier(seed=2020)\n",
    "tree.fit(train_results, labels)\n",
    "\n",
    "test_results = []\n",
    "test_labels = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for img in test_loader:\n",
    "        img = img.to(device)\n",
    "        test_results.append(model(img).cpu().numpy())\n",
    "        test_labels.append(tree.predict(model(img).cpu().numpy()))\n",
    "        \n",
    "test_results = np.concatenate(test_results)\n",
    "test_labels = np.concatenate(test_labels)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15, 10), facecolor=\"azure\")\n",
    "for label in np.unique(test_labels):        \n",
    "   \n",
    "    tmp = test_results[test_labels==label]\n",
    "    plt.scatter(tmp[:, 0], tmp[:, 1], label=classes[label])\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# accuracy \n",
    "true_=(tree.predict(test_results)==test_labels).sum()\n",
    "len_ = len(test_labels)\n",
    "print(\"Accuracy :{}%\".format((true_/len_)*100)) ##100%"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2b06eea86720a957e96073e5e3b3d48d33ced1ae9c666ea436625d157f43d685"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
